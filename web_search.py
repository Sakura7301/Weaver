"""
ç½‘ç»œæœç´¢åŠŸèƒ½æ¨¡å—
"""
import requests
from datetime import datetime
from bs4 import BeautifulSoup

def fetch_webpage(url, timeout=5):
    """çˆ¬å–ç½‘é¡µå¹¶æå–æ–‡æœ¬å†…å®¹
    
    Args:
        url: ç½‘é¡µURL
        timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    
    Returns:
        str: æå–çš„æ–‡æœ¬å†…å®¹æˆ–é”™è¯¯ä¿¡æ¯
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        resp = requests.get(url, headers=headers, timeout=timeout, verify=False)
        resp.encoding = resp.apparent_encoding
        
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        # ç§»é™¤scriptã€styleç­‰æ ‡ç­¾
        for tag in soup(['script', 'style', 'nav', 'footer', 'header', 'aside']):
            tag.decompose()
        
        # æå–æ–‡æœ¬
        text = soup.get_text(separator='\n', strip=True)
        
        # æ¸…ç†å¤šä½™ç©ºè¡Œ
        lines = [line.strip() for line in text.split('\n') if line.strip()]
        text = '\n'.join(lines)
        
        # é™åˆ¶é•¿åº¦ï¼ˆå‰1500å­—ç¬¦ï¼‰
        text = text[:1500]
        
        return text
        
    except Exception as e:
        return f"çˆ¬å–å¤±è´¥: {str(e)}"

def web_search(query, searxng_url, max_fetch=2):
    """
    æœç´¢ç½‘ç»œå¹¶è¿”å›ç»“æ„åŒ–ç»“æœ
    
    Args:
        query: æœç´¢å…³é”®è¯
        searxng_url: SearXNGæœåŠ¡å™¨åœ°å€
        max_fetch: æœ€å¤§çˆ¬å–ç½‘é¡µæ•°é‡
    
    Returns:
        dict: ç»“æ„åŒ–æœç´¢ç»“æœ
    """
    print(f"\nğŸ” æ­£åœ¨æœç´¢: {query}")
    
    try:
        # è°ƒç”¨ SearXNG
        resp = requests.get(
            f"{searxng_url}/search",
            params={"q": query, "format": "json"},
            verify=False,
            timeout=10
        )
        
        data = resp.json()
        raw_results = data.get("results", [])
        
        if not raw_results:
            print("âŒ æ²¡æœ‰æœç´¢ç»“æœ")
            return {
                "success": False,
                "query": query,
                "results": [],
                "message": "æœªæ‰¾åˆ°ç›¸å…³ç»“æœ"
            }
        
        print(f"âœ… æ‰¾åˆ° {len(raw_results)} æ¡ç»“æœ")
        
        # ç»“æ„åŒ–å¤„ç†
        structured_results = []
        
        for i, r in enumerate(raw_results[:5]):  # å–å‰5æ¡
            result = {
                "title": r.get('title', ''),
                "url": r.get('url', ''),
                "snippet": r.get('content', '')[:200],
                "engine": ', '.join(r.get('engines', [])),
                "content": None
            }
            
            # çˆ¬å–å‰Nä¸ªç½‘é¡µçš„å†…å®¹
            if i < max_fetch:
                print(f"  ğŸ“„ çˆ¬å– [{i+1}]: {result['title'][:40]}...")
                content = fetch_webpage(result['url'])
                result['content'] = content
                print(f"     âœ… æå– {len(content)} å­—ç¬¦")
            
            structured_results.append(result)
        
        print(f"âœ… æœç´¢å®Œæˆï¼Œè¿”å› {len(structured_results)} æ¡ç»“æ„åŒ–ç»“æœ\n")
        
        return {
            "success": True,
            "query": query,
            "results": structured_results,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        print(f"âŒ æœç´¢å¤±è´¥: {e}")
        return {
            "success": False,
            "query": query,
            "results": [],
            "error": str(e)
        }