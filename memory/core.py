import hashlib
from datetime import datetime, timedelta 
from openai import OpenAI

from config import (
    init_filesystem,
    MEMORY_FILE,
    DAILY_MEMORY_DIR,
    EMBEDDING_MODEL,
    MERGE_SIMILARITY_THRESHOLD,
    MERGE_INTERVAL_DAYS,
    LAST_MERGE_FILE
)
from .database import DatabaseManager
from .embedding import EmbeddingManager
from .search import SearchEngine
from .merge import MergeManager


class MemorySystem:
    def __init__(self, api_key, base_url, embedding_model=None, merge_threshold=None, merge_interval_days=None, memory_dir=None):
        """
        åˆå§‹åŒ–è®°å¿†ç³»ç»Ÿ
        
        Args:
            api_key: APIå¯†é’¥
            base_url: APIåœ°å€
            embedding_model: åµŒå…¥æ¨¡å‹åç§°
            merge_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0.5-0.8ï¼‰
            merge_interval_days: å®šæœŸæ•´ç†é—´éš”ï¼ˆå¤©ï¼‰
            memory_dir: è®°å¿†å­˜å‚¨è·¯å¾„ï¼ˆé»˜è®¤ ~/.ai_memoryï¼‰
        """
        self.api_key = api_key
        self.base_url = base_url
        self.embedding_model = embedding_model or EMBEDDING_MODEL
        self.merge_threshold = merge_threshold or MERGE_SIMILARITY_THRESHOLD
        self.merge_interval_days = merge_interval_days or MERGE_INTERVAL_DAYS
        
        # åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
        self.client = OpenAI(api_key=api_key, base_url=base_url)
        
        # åˆå§‹åŒ–æ–‡ä»¶ç³»ç»Ÿ
        self.paths = init_filesystem()
        
        # åˆå§‹åŒ–å„æ¨¡å—
        self.db_manager = DatabaseManager(self.paths["memory_db"])
        self.embedding_manager = EmbeddingManager(self.client, self.embedding_model, self.db_manager)
        self.search_engine = SearchEngine(self.embedding_manager, self.db_manager)
        self.merge_manager = MergeManager(
            self.client, self.db_manager, self.embedding_manager,
            self.merge_threshold, self.merge_interval_days
        )
    
    def save_memory(self, text, memory_type="short"):
        """
        æ™ºèƒ½ä¿å­˜è®°å¿†
        
        Args:
            text: è¦ä¿å­˜çš„æ–‡æœ¬
            memory_type: "long" (MEMORY.md + æ•°æ®åº“) æˆ– "short" (ä»… daily/æ—¥æœŸ.md)
        """
        if memory_type == "long":
            # === é•¿æœŸè®°å¿†ï¼šå®æ—¶å»é‡ + å‘é‡åŒ– ===
            similar = self.search_engine.search(text, top_k=3, min_score=self.merge_threshold)
            
            if similar:
                print(f"âš ï¸  æ£€æµ‹åˆ° {len(similar)} æ¡ç›¸ä¼¼è®°å¿†ï¼Œæ™ºèƒ½åˆå¹¶ä¸­...")
                
                try:
                    # 1. æ™ºèƒ½åˆå¹¶
                    merged_text = self.merge_manager.smart_merge(text, similar)
                    
                    # 2. åˆ é™¤æ—§è®°å¿†ï¼ˆæ–‡ä»¶ï¼‰
                    content = MEMORY_FILE.read_text(encoding='utf-8')
                    lines = content.split('\n')
                    new_lines = []
                    
                    for line in lines:
                        # æ£€æŸ¥æ˜¯å¦åŒ…å«è¦åˆ é™¤çš„æ—§è®°å¿†
                        should_keep = True
                        for s in similar:
                            if s['text'] in line:
                                should_keep = False
                                break
                        if should_keep:
                            new_lines.append(line)
                    
                    content = '\n'.join(new_lines)
                    MEMORY_FILE.write_text(content, encoding='utf-8')
                    
                    # 3. å†™å…¥åˆå¹¶åçš„æ–°è®°å¿†
                    with open(MEMORY_FILE, 'a', encoding='utf-8') as f:
                        f.write(f"\n- {merged_text} (æ›´æ–°äº {datetime.now().strftime('%Y-%m-%d %H:%M')})\n")
                    
                    # 4. åˆ é™¤æ—§å‘é‡
                    for s in similar:
                        self.db_manager.delete_chunk(s['id'])
                    
                    print(f"âœ… å·²åˆå¹¶æ›´æ–°: {merged_text[:60]}...")
                    text = merged_text
                    
                except Exception as e:
                    print(f"âš ï¸  åˆå¹¶å¤±è´¥ï¼Œé™çº§ä¸ºè¿½åŠ æ¨¡å¼: {e}")
                    with open(MEMORY_FILE, 'a', encoding='utf-8') as f:
                        f.write(f"\n- {text} (è®°å½•äº {datetime.now().strftime('%Y-%m-%d %H:%M')})\n")
            else:
                # æ²¡æœ‰ç›¸ä¼¼å†…å®¹ï¼Œç›´æ¥è¿½åŠ 
                with open(MEMORY_FILE, 'a', encoding='utf-8') as f:
                    f.write(f"\n- {text} (è®°å½•äº {datetime.now().strftime('%Y-%m-%d %H:%M')})\n")
            
            # === å‘é‡åŒ–å¹¶å­˜å…¥æ•°æ®åº“ï¼ˆåªæœ‰é•¿æœŸè®°å¿†ï¼‰ ===
            embedding = self.embedding_manager.get_embedding(text)
            if embedding:
                chunk_id = hashlib.md5(f"{text}{datetime.now()}".encode()).hexdigest()
                self.db_manager.save_chunk(chunk_id, "MEMORY.md", text, embedding)
            
            print(f"ğŸ’¾ å·²ä¿å­˜é•¿æœŸè®°å¿†ï¼ˆå·²å‘é‡åŒ–ï¼‰")
                
        else:
            # === çŸ­æœŸè®°å¿†ï¼šåªå­˜æ–‡ä»¶ï¼Œä¸å‘é‡åŒ– ===
            today = datetime.now().strftime("%Y-%m-%d")
            daily_file = DAILY_MEMORY_DIR / f"{today}.md"
            
            if not daily_file.exists():
                daily_file.write_text(f"# {today} è®°å¿†æ—¥å¿—\n\n", encoding='utf-8')
            
            with open(daily_file, 'a', encoding='utf-8') as f:
                f.write(f"{text}\n\n")  # ä¿æŒå®Œæ•´ä¸Šä¸‹æ–‡æ ¼å¼
            
            print(f"ğŸ’¾ å·²ä¿å­˜çŸ­æœŸè®°å¿†ï¼ˆä»…æ–‡ä»¶ï¼Œæœªå‘é‡åŒ–ï¼‰{daily_file}")
    
    def search_memory(self, query, top_k=5, min_score=0.3):
        """
        æœç´¢è®°å¿†
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            top_k: è¿”å›ç»“æœæ•°é‡
            min_score: æœ€ä½åˆ†æ•°é˜ˆå€¼
        
        Returns:
            [{"text": "...", "path": "...", "score": 0.85}, ...]
        """
        return self.search_engine.search(query, top_k, min_score)
    
    def should_merge(self):
        """æ£€æŸ¥æ˜¯å¦éœ€è¦å®šæœŸæ•´ç†"""
        return self.merge_manager.should_merge()
    
    def deep_merge_all(self):
        """æ·±åº¦æ•´ç†æ‰€æœ‰é•¿æœŸè®°å¿†ï¼ˆå®šæœŸä»»åŠ¡ï¼‰"""
        self.merge_manager.deep_merge_all()
    
    def check_and_auto_merge(self):
        """å¯åŠ¨æ—¶æ£€æŸ¥å¹¶è‡ªåŠ¨æ•´ç†"""
        if self.should_merge():
            days = 999 if not LAST_MERGE_FILE.exists() else (datetime.now() - datetime.fromtimestamp(LAST_MERGE_FILE.stat().st_mtime)).days
            
            print(f"\nâ° è·ç¦»ä¸Šæ¬¡æ•´ç†å·²è¿‡ {days} å¤©ï¼Œè§¦å‘è‡ªåŠ¨æ•´ç†...")
            self.deep_merge_all()
        else:
            last_merge = datetime.fromtimestamp(LAST_MERGE_FILE.stat().st_mtime)
            next_merge = last_merge + timedelta(days=self.merge_interval_days)  # ä¿®å¤è¿™é‡Œ
            days_left = (next_merge - datetime.now()).days
            print(f"âœ… è®°å¿†çŠ¶æ€è‰¯å¥½ï¼Œä¸‹æ¬¡æ•´ç†: {next_merge.strftime('%Y-%m-%d')} ({days_left}å¤©å)")
    
    def get_long_term_memory(self):
        """è¯»å–å®Œæ•´é•¿æœŸè®°å¿†ï¼ˆç”¨äºæ³¨å…¥ä¸Šä¸‹æ–‡ï¼‰"""
        if MEMORY_FILE.exists():
            return MEMORY_FILE.read_text(encoding='utf-8')
        return ""
    
    def get_stats(self):
        """è·å–è®°å¿†ç»Ÿè®¡ä¿¡æ¯"""
        # é•¿æœŸè®°å¿†æ•°ï¼ˆæ•°æ®åº“ä¸­çš„ï¼‰
        long_term = self.db_manager.get_chunk_count_by_path("MEMORY.md")
        
        # çŸ­æœŸè®°å¿†æ–‡ä»¶æ•°
        daily_files = len(list(DAILY_MEMORY_DIR.glob("*.md")))
        
        # é•¿æœŸè®°å¿†æ–‡ä»¶è¡Œæ•°
        if MEMORY_FILE.exists():
            content = MEMORY_FILE.read_text(encoding='utf-8')
            long_term_lines = len([l for l in content.split('\n') if l.strip().startswith('- ')])
        else:
            long_term_lines = 0
        
        return {
            "long_term_indexed": long_term,  # æ•°æ®åº“ä¸­çš„é•¿æœŸè®°å¿†
            "long_term_total": long_term_lines,  # æ–‡ä»¶ä¸­çš„é•¿æœŸè®°å¿†
            "daily_files": daily_files  # çŸ­æœŸè®°å¿†å¤©æ•°
        }
    
    def auto_index(self):
        """è‡ªåŠ¨ç´¢å¼•é•¿æœŸè®°å¿†æ–‡ä»¶ï¼ˆçŸ­æœŸè®°å¿†ä¸ç´¢å¼•ï¼‰"""
        print("ğŸ“š ç´¢å¼•é•¿æœŸè®°å¿†...")
        
        if not MEMORY_FILE.exists():
            print("âš ï¸  é•¿æœŸè®°å¿†æ–‡ä»¶ä¸å­˜åœ¨")
            return
        
        content = MEMORY_FILE.read_text(encoding='utf-8')
        
        # æŒ‰æ®µè½åˆ†å—
        paragraphs = [p.strip() for p in content.split('\n') if p.strip() and p.startswith('- ')]
        
        for para in paragraphs:
            # å»é™¤å‰ç¼€ "- "
            para_clean = para.lstrip('- ').strip()
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            para_hash = hashlib.md5(para_clean.encode()).hexdigest()
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼ˆç®€åŒ–æ£€æŸ¥ï¼‰
            chunks = self.db_manager.get_all_chunks()
            exists = any(chunk_id == para_hash for chunk_id, _, _, _ in chunks)
            
            if not exists:
                embedding = self.embedding_manager.get_embedding(para_clean)
                if embedding:
                    self.db_manager.save_chunk(para_hash, "MEMORY.md", para_clean, embedding)
        
        print(f"âœ… å·²ç´¢å¼• {len(paragraphs)} æ¡é•¿æœŸè®°å¿†")